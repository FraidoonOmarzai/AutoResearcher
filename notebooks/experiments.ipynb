{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4ea78e",
   "metadata": {},
   "source": [
    "<h1 align=center> Autonomous Research Assistant Agents </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea0b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import os\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from typing import Annotated, TypedDict, List, Optional\n",
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0715ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_groq import ChatGroq\n",
    "\n",
    "# llm = ChatGroq(model_name=\"deepseek-r1-distill-llama-70b\")\n",
    "\n",
    "# results = llm.invoke(\"tell me about multi agent types?\")\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17126bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BmQwmvIOF9WDcloZpFW5Ka5v9JLVD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e398f637-3720-4bd8-9070-ba954f90bec1-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize LLM (use gpt-4 or gpt-3.5-turbo)\n",
    "llm2 = ChatOpenAI(model=\"gpt-4\", temperature=0.2)\n",
    "llm2.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3f356",
   "metadata": {},
   "source": [
    "`Excellent progress â€” you now have:`\n",
    "\n",
    "âœ… Search Agent â€“ finds relevant papers\n",
    "\n",
    "âœ… Filter Agent â€“ selects the most suitable ones\n",
    "\n",
    "âœ… Summarizer Agent â€“ produces readable, concise summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e70af939",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Search Agent\n",
    "\n",
    "def get_arxiv_papers(topic: str, max_results: int = 5) -> list:\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "\n",
    "    papers = []\n",
    "    for result in search.results():\n",
    "        paper_info = {\n",
    "            \"title\": result.title,\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"summary\": result.summary.strip().replace(\"\\n\", \" \"),\n",
    "            \"url\": result.entry_id,\n",
    "            \"pdf_url\": result.pdf_url,\n",
    "            \"published\": str(result.published.date()),\n",
    "            \"year\": result.published.year\n",
    "        }\n",
    "        papers.append(paper_info)\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99483f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44787\\AppData\\Local\\Temp\\ipykernel_24804\\201622925.py:9: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Paper 1: Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration\n",
      "ðŸ‘¨â€ðŸ”¬ Authors: Kexin Ding, Mu Zhou, Akshay Chaudhari, Shaoting Zhang, Dimitris N. Metaxas\n",
      "ðŸ—“ï¸ Published: 2025-05-02\n",
      "ðŸ”— URL: http://arxiv.org/abs/2505.02848v1\n",
      "ðŸ“„ Abstract:\n",
      "The wide exploration of large language models (LLMs) raises the awareness of alignment between\n",
      "healthcare stakeholder preferences and model outputs. This alignment becomes a crucial foundation to\n",
      "empower the healthcare workflow effectively, safely, and responsibly. Yet the varying behaviors of\n",
      "LLMs may not always match with healthcare stakeholders' knowledge, demands, and values. To enable a\n",
      "human-AI alignment, healthcare stakeholders will need to perform essential roles in guiding and\n",
      "enhancing...\n",
      "\n",
      "ðŸ“„ Paper 2: An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT\n",
      "ðŸ‘¨â€ðŸ”¬ Authors: Shyni Sharaf, V. S. Anoop\n",
      "ðŸ—“ï¸ Published: 2023-10-11\n",
      "ðŸ”— URL: http://arxiv.org/abs/2310.07282v2\n",
      "ðŸ“„ Abstract:\n",
      "This paper conducts a comprehensive investigation into applying large language models, particularly\n",
      "on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing\n",
      "(NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face.\n",
      "Following that, this research explores the path that led to the incorporation of BioBERT into\n",
      "healthcare applications, highlighting its suitability for addressing the specific requirements of\n",
      "tasks r...\n",
      "\n",
      "ðŸ“„ Paper 3: The Role of Language Models in Modern Healthcare: A Comprehensive Review\n",
      "ðŸ‘¨â€ðŸ”¬ Authors: Amna Khalid, Ayma Khalid, Umar Khalid\n",
      "ðŸ—“ï¸ Published: 2024-09-25\n",
      "ðŸ”— URL: http://arxiv.org/abs/2409.16860v1\n",
      "ðŸ“„ Abstract:\n",
      "The application of large language models (LLMs) in healthcare has gained significant attention due\n",
      "to their ability to process complex medical data and provide insights for clinical decision-making.\n",
      "These models have demonstrated substantial capabilities in understanding and generating natural\n",
      "language, which is crucial for medical documentation, diagnostics, and patient interaction. This\n",
      "review examines the trajectory of language models from their early stages to the current state-of-\n",
      "the-art LL...\n",
      "\n",
      "ðŸ“„ Paper 4: Large language models in healthcare and medical domain: A review\n",
      "ðŸ‘¨â€ðŸ”¬ Authors: Zabir Al Nazi, Wei Peng\n",
      "ðŸ—“ï¸ Published: 2023-12-12\n",
      "ðŸ”— URL: http://arxiv.org/abs/2401.06775v2\n",
      "ðŸ“„ Abstract:\n",
      "The deployment of large language models (LLMs) within the healthcare sector has sparked both\n",
      "enthusiasm and apprehension. These models exhibit the remarkable capability to provide proficient\n",
      "responses to free-text queries, demonstrating a nuanced understanding of professional medical\n",
      "knowledge. This comprehensive survey delves into the functionalities of existing LLMs designed for\n",
      "healthcare applications, elucidating the trajectory of their development, starting from traditional\n",
      "Pretrained Langu...\n",
      "\n",
      "ðŸ“„ Paper 5: Developing Healthcare Language Model Embedding Spaces\n",
      "ðŸ‘¨â€ðŸ”¬ Authors: Niall Taylor, Dan Schofield, Andrey Kormilitzin, Dan W Joyce, Alejo Nevado-Holgado\n",
      "ðŸ—“ï¸ Published: 2024-03-28\n",
      "ðŸ”— URL: http://arxiv.org/abs/2403.19802v1\n",
      "ðŸ“„ Abstract:\n",
      "Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare\n",
      "focused text. We explore specialized pre-training to adapt smaller LLMs to different healthcare\n",
      "datasets. Three methods are assessed: traditional masked language modeling, Deep Contrastive\n",
      "Learning for Unsupervised Textual Representations (DeCLUTR), and a novel pre-training objective\n",
      "utilizing metadata categories from the healthcare settings. These schemes are evaluated on\n",
      "downstream document classi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "user_topic = \"Large Language Models in Healthcare\"\n",
    "papers = get_arxiv_papers(user_topic)\n",
    "\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=100)\n",
    "\n",
    "for i, paper in enumerate(papers):\n",
    "    print(f\"ðŸ“„ Paper {i+1}: {paper['title']}\")\n",
    "    print(f\"ðŸ‘¨â€ðŸ”¬ Authors: {', '.join(paper['authors'])}\")\n",
    "    print(f\"ðŸ—“ï¸ Published: {paper['published']}\")\n",
    "    print(f\"ðŸ”— URL: {paper['url']}\")\n",
    "    print(\"ðŸ“„ Abstract:\")\n",
    "    print(wrapper.fill(paper['summary'][:500]) + \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05427b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c57acbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.Filter Agent\n",
    "\n",
    "filter_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"papers\"],\n",
    "    template=\"\"\"\n",
    "        Given the topic: \"{topic}\"\n",
    "\n",
    "        Here are 5 research papers (title and abstract):\n",
    "        {papers}\n",
    "\n",
    "        From these, select the top 3 papers that are **most relevant and recent** to the topic.\n",
    "        Output them in a JSON format like:\n",
    "        [\n",
    "          {{ \"title\": \"...\", \"summary\": \"...\", \"year\": 2023, \"why_selected\": \"...\" }},\n",
    "          ...\n",
    "        ]\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "filter_chain = filter_prompt | llm2\n",
    "\n",
    "filtered = filter_chain.invoke({\n",
    "    \"topic\": user_topic,\n",
    "    \"papers\": \"\\n\\n\".join([f\"Title: {p['title']}\\nAbstract: {p['summary']}\" for p in papers])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fbfa3dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Paper 1: Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration\n",
      "ðŸ‘¨â€ðŸ”¬ Authors: N/A (not included here)\n",
      "ðŸ—“ï¸ Year: 2023\n",
      "ðŸ“Œ Why Selected: This paper is selected because it discusses the alignment between large language models and healthcare stakeholders, which is a crucial aspect of integrating AI in healthcare. It also provides an outlook on enhancing this alignment, making it highly relevant to the topic.\n",
      "ðŸ“„ Summary: The wide exploration of large language models (LLMs) raises the awareness of alignment between\n",
      "healthcare stakeholder preferences and model outputs. This alignment becomes a crucial foundation to\n",
      "empower the healthcare workflow effectively, safely, and responsibly. Yet the varying behaviors of\n",
      "LLMs may not always match with healthcare stakeholders' knowledge, demands, and values. To enable a\n",
      "human-AI alignment, healthcare stakeholders will need to perform essential roles in guiding and\n",
      "enhancing...\n",
      "\n",
      "ðŸ“„ Paper 2: An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT\n",
      "ðŸ‘¨â€ðŸ”¬ Authors: N/A (not included here)\n",
      "ðŸ—“ï¸ Year: 2023\n",
      "ðŸ“Œ Why Selected: This paper is selected because it provides a comprehensive analysis of the application of large language models, specifically BioBERT, in healthcare. It also outlines a systematic methodology for fine-tuning these models to meet the unique needs of the healthcare domain, making it highly relevant to the topic.\n",
      "ðŸ“„ Summary: This paper conducts a comprehensive investigation into applying large language models, particularly\n",
      "on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing\n",
      "(NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face.\n",
      "Following that, this research explores the path that led to the incorporation of BioBERT into\n",
      "healthcare applications, highlighting its suitability for addressing the specific requirements of\n",
      "tasks r...\n",
      "\n",
      "ðŸ“„ Paper 3: Developing Healthcare Language Model Embedding Spaces\n",
      "ðŸ‘¨â€ðŸ”¬ Authors: N/A (not included here)\n",
      "ðŸ—“ï¸ Year: 2023\n",
      "ðŸ“Œ Why Selected: This paper is selected because it explores specialized pre-training to adapt large language models to different healthcare datasets. It also evaluates these models on downstream document classification tasks, making it highly relevant to the topic.\n",
      "ðŸ“„ Summary: Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare\n",
      "focused text. We explore specialized pre-training to adapt smaller LLMs to different healthcare\n",
      "datasets. Three methods are assessed: traditional masked language modeling, Deep Contrastive\n",
      "Learning for Unsupervised Textual Representations (DeCLUTR), and a novel pre-training objective\n",
      "utilizing metadata categories from the healthcare settings. These schemes are evaluated on\n",
      "downstream document classi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Step 1: Extract JSON string from the content\n",
    "json_str = filtered.content\n",
    "\n",
    "# Step 2: Parse JSON string into Python list\n",
    "filtered_papers = json.loads(json_str)\n",
    "\n",
    "# Step 3: Now you can iterate over papers and access dict keys\n",
    "for i, paper in enumerate(filtered_papers):\n",
    "    print(f\"ðŸ“„ Paper {i+1}: {paper['title']}\")\n",
    "    print(f\"ðŸ‘¨â€ðŸ”¬ Authors: N/A (not included here)\")\n",
    "    print(f\"ðŸ—“ï¸ Year: {paper['year']}\")\n",
    "    print(f\"ðŸ“Œ Why Selected: {paper['why_selected']}\")\n",
    "    print(f\"ðŸ“„ Summary: {wrapper.fill(paper['summary'][:500])}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "501b509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Summarization Agent\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"title\", \"abstract\"],\n",
    "    template=\"\"\"\n",
    "        You are a research assistant. Given the title and abstract of a paper, write a clear and concise summary (~100 words) in plain English.\n",
    "\n",
    "        Title: {title}\n",
    "        Abstract: {abstract}\n",
    "\n",
    "        Summary:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "summary_chain = summary_prompt | llm2\n",
    "\n",
    "summaries = []\n",
    "for paper in filtered_papers:\n",
    "    summary = summary_chain.invoke({\n",
    "        \"title\": paper[\"title\"],\n",
    "        \"abstract\": paper[\"summary\"]\n",
    "    })\n",
    "    summaries.append({\n",
    "        \"title\": paper[\"title\"],\n",
    "        \"summary\": summary,\n",
    "        \"link\": paper.get(\"url\")\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "623bf51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Paper 1: Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration\n",
      "ðŸ”— Link: No link provided\n",
      "ðŸ“„ Summary: The wide exploration of large language models (LLMs) raises the awareness of alignment between\n",
      "healthcare stakeholder preferences and model outputs. This alignment becomes a crucial foundation to\n",
      "empower the healthcare workflow effectively, safely, and responsibly. Yet the varying behaviors of\n",
      "LLMs may not always match with healthcare stakeholders' knowledge, demands, and values. To enable a\n",
      "human-AI alignment, healthcare stakeholders will need to perform essential roles in guiding and\n",
      "enhancing...\n",
      "\n",
      "ðŸ“„ Paper 2: An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT\n",
      "ðŸ”— Link: No link provided\n",
      "ðŸ“„ Summary: This paper conducts a comprehensive investigation into applying large language models, particularly\n",
      "on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing\n",
      "(NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face.\n",
      "Following that, this research explores the path that led to the incorporation of BioBERT into\n",
      "healthcare applications, highlighting its suitability for addressing the specific requirements of\n",
      "tasks r...\n",
      "\n",
      "ðŸ“„ Paper 3: Developing Healthcare Language Model Embedding Spaces\n",
      "ðŸ”— Link: No link provided\n",
      "ðŸ“„ Summary: Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare\n",
      "focused text. We explore specialized pre-training to adapt smaller LLMs to different healthcare\n",
      "datasets. Three methods are assessed: traditional masked language modeling, Deep Contrastive\n",
      "Learning for Unsupervised Textual Representations (DeCLUTR), and a novel pre-training objective\n",
      "utilizing metadata categories from the healthcare settings. These schemes are evaluated on\n",
      "downstream document classi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, paper in enumerate(papers):\n",
    "    title = paper[\"title\"]\n",
    "    summary = paper[\"summary\"]\n",
    "    link = paper.get(\"link\", \"No link provided\")\n",
    "\n",
    "    print(f\"ðŸ“„ Paper {i+1}: {title}\")\n",
    "    print(f\"ðŸ”— Link: {link}\")\n",
    "    print(f\"ðŸ“„ Summary: {wrapper.fill(summary[:500])}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "841826bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "def save_summary_pdf(papers, filename=\"summary.pdf\"):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    \n",
    "    for i, paper in enumerate(papers):\n",
    "        pdf.multi_cell(0, 10, f\"{i+1}. {paper['title']}\", align='L')\n",
    "        pdf.multi_cell(0, 10, paper['summary'].content if hasattr(paper['summary'], 'content') else str(paper['summary']))\n",
    "        pdf.ln(10)\n",
    "    \n",
    "    pdf.output(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2bff0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_summary_pdf(summaries, f\"{user_topic}+.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bb7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f9bac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain prompt template\n",
    "summary_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI research assistant. Read the abstract below and summarize the key points.\n",
    "\n",
    "Abstract:\n",
    "\"{abstract}\"\n",
    "\n",
    "Summarize in 3 bullet points.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "summarizer_chain = summary_prompt | llm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f775409",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_papers = []\n",
    "\n",
    "for paper in papers:\n",
    "    abstract = paper[\"summary\"]\n",
    "    summary = summarizer_chain.invoke({\"abstract\": abstract})\n",
    "\n",
    "    paper_with_summary = {\n",
    "        **paper,\n",
    "        \"summary_bullets\": summary.content\n",
    "    }\n",
    "    summarized_papers.append(paper_with_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d460c22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Paper 1: Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration\n",
      "ðŸ“ Summary:\n",
      "- The use of large language models (LLMs) in healthcare requires alignment with the preferences, knowledge, and values of healthcare stakeholders to ensure effective, safe, and responsible workflows.\n",
      "- Healthcare professionals need to be involved in all stages of LLM adoption, including data curation, model training, and inference, to ensure human-AI alignment.\n",
      "- The review discusses methods to improve alignment between healthcare stakeholders and LLMs, emphasizing the need for better integration of healthcare knowledge, task understanding, and human guidance to build trustworthy healthcare applications.\n",
      "ðŸ”— URL: http://arxiv.org/abs/2505.02848v1\n",
      "================================================================================\n",
      "\n",
      "ðŸ“˜ Paper 2: An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT\n",
      "ðŸ“ Summary:\n",
      "- The paper investigates the application of large language models, specifically BioBERT, in healthcare, discussing its suitability for tasks related to biomedical text mining and outlining a systematic methodology for fine-tuning it to meet healthcare-specific needs. This includes data gathering, annotation, and specialized preprocessing techniques.\n",
      "- The research also covers model evaluation, focusing on healthcare benchmarks and functions like processing natural language in biomedical, question-answering, clinical document classification, and medical entity recognition. It also explores ways to improve the model's interpretability and compares its performance with existing healthcare-focused language models.\n",
      "- The paper highlights the benefits of incorporating BioBERT into healthcare, such as improved clinical decision support and efficient information retrieval, but also acknowledges challenges such as data privacy concerns, resource-intensive requirements, and the need for model customization to suit diverse healthcare domains.\n",
      "ðŸ”— URL: http://arxiv.org/abs/2310.07282v2\n",
      "================================================================================\n",
      "\n",
      "ðŸ“˜ Paper 3: The Role of Language Models in Modern Healthcare: A Comprehensive Review\n",
      "ðŸ“ Summary:\n",
      "- Large language models (LLMs) are gaining prominence in healthcare due to their ability to process complex medical data and aid in clinical decision-making, particularly in areas like medical documentation, diagnostics, and patient interaction.\n",
      "- The review traces the evolution of these language models, emphasizing their strengths in healthcare applications, while also addressing challenges such as data privacy, bias, and ethical considerations.\n",
      "- The potential of LLMs to improve healthcare delivery is discussed, along with the necessary steps to ensure their ethical and effective integration into medical practice.\n",
      "ðŸ”— URL: http://arxiv.org/abs/2409.16860v1\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, paper in enumerate(summarized_papers):\n",
    "    print(f\"ðŸ“˜ Paper {i+1}: {paper['title']}\")\n",
    "    print(f\"ðŸ“ Summary:\\n{paper['summary_bullets']}\")\n",
    "    print(f\"ðŸ”— URL: {paper['url']}\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf9cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f\"{topic}.json\", \"w\") as f:\n",
    "    json.dump(summarized_papers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab597d0d",
   "metadata": {},
   "source": [
    "`Define Shared State`\n",
    "\n",
    "- LangGraph uses a shared dictionary-like state that agents can read/write.\n",
    "- Define what your state will contain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96981930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    papers: Optional[List[dict]]\n",
    "    summaries: Optional[List[dict]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "709976f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return goto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0ee262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt(instruction: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "        f\"\\n{instruction}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13576855",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def arxiv1(topic: str, max_results: int = 2) -> str:\n",
    "    \"\"\"Fetches relevant paper abstracts from arXiv based on a topic.\"\"\"\n",
    "    papers = get_arxiv_papers(topic, max_results)\n",
    "    output = \"\"\n",
    "    for i, paper in enumerate(papers):\n",
    "        output += f\"\\n[{i+1}] {paper['title']} ({paper['published']})\\n\"\n",
    "        output += f\"Authors: {', '.join(paper['authors'])}\\n\"\n",
    "        output += f\"Abstract: {paper['summary'][:500]}...\\n\"\n",
    "        output += f\"URL: {paper['url']}\\n\\n\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d05d1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_agent = create_react_agent(\n",
    "    llm2,\n",
    "    tools=[arxiv1],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only do research. You are working with a summarizer colleague.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c3c6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def summarize_abstract(abstract: str) -> str:\n",
    "    \"\"\"\n",
    "    Summarizes a research paper abstract into 3 concise bullet points.\n",
    "    \"\"\"\n",
    "\n",
    "    summary_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    You are an AI research assistant. Read the abstract below and summarize the key points.\n",
    "\n",
    "    Abstract:\n",
    "    \"{abstract}\"\n",
    "\n",
    "    Summarize in 3 bullet points.\n",
    "    \"\"\")\n",
    "\n",
    "    response = summary_prompt | llm2\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7f548a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_agent = create_react_agent(\n",
    "    llm2,\n",
    "    tools=[summarize_abstract],\n",
    "    prompt=make_system_prompt(\n",
    "        \"You can only summarize abstracts. Your teammate provides the abstracts.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd93fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_node(state: MessagesState) -> Command[Literal[\"summarizer\", END]]:\n",
    "\n",
    "    result = research_agent.invoke(state)\n",
    "\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"summarizer\")\n",
    "\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "\n",
    "    return Command(update={\"messages\": result[\"messages\"]}, goto=goto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae2d119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer_node(state: MessagesState) -> Command[Literal[\"researcher\", END]]:\n",
    "\n",
    "    result = summarized_agent.invoke(state)\n",
    "\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
    "\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"summarizer\")\n",
    "\n",
    "    return Command(update={\"messages\": result[\"messages\"]}, goto=goto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a241b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"summarizer\", summarizer_node)\n",
    "\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46cb52e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d629430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44787\\AppData\\Local\\Temp\\ipykernel_24804\\3121392185.py:9: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='get the research paper in transformer', additional_kwargs={}, response_metadata={}, id='07c6511d-f92b-47b2-a42c-b963a728f083'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao1MjJ7fSCGPvtbAkykWBiEz', 'function': {'arguments': '{\\n  \"topic\": \"transformer\",\\n  \"max_results\": 5\\n}', 'name': 'arxiv1'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 173, 'total_tokens': 198, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BmQxVpkjNxxJZJkROoapL1TrhbeyG', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--0059b588-16dc-4c61-a3d3-87e3866b58c4-0', tool_calls=[{'name': 'arxiv1', 'args': {'topic': 'transformer', 'max_results': 5}, 'id': 'call_ao1MjJ7fSCGPvtbAkykWBiEz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 173, 'output_tokens': 25, 'total_tokens': 198, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"\\n[1] The Xi-transform for conformally flat space-time (2006-12-01)\\nAuthors: George Sparling\\nAbstract: The Xi-transform is a new spinor transform arising naturally in Einstein's\\ngeneral relativity. Here the example of conformally flat space-time is\\ndiscussed in detail. In particular it is shown that for this case, the\\ntransform coincides with two other naturally defined transforms: one a\\ntwo-variable transform on the Lie group SU(2, C), the other a transform on the\\nspace of null split octaves. The key properties of the transform are developed....\\nURL: http://arxiv.org/abs/gr-qc/0612006v1\\n\\n\\n[2] Multiple basic hypergeometric transformation formulas arising from the balanced duality transformation (2013-10-08)\\nAuthors: Yasushi Kajihara\\nAbstract: Some multiple hypergeometric transformation formulas arising from the\\nbalanced du- ality transformation formula are discussed through the symmetry.\\nDerivations of some transformation formulas with different dimensions are given\\nby taking certain limits of the balanced duality transformation. By combining\\nsome of them, some transformation formulas for $A_n$ basic hypergeometric\\nseries is given. They include some generalizations of Watson, Sears and ${}_8\\nW_7$ transformations....\\nURL: http://arxiv.org/abs/1310.1984v2\\n\\n\\n[3] The Fourier and Hilbert transforms under the Bargmann transform (2016-05-27)\\nAuthors: Xing-Tang Dong, Kehe Zhu\\nAbstract: There is a canonical unitary transformation from $L^2(\\\\R)$ onto the Fock\\nspace $F^2$, called the Bargmann transform. We study the action of the Bargmann\\ntransform on several classical integral operators on $L^2(\\\\R)$, including the\\nfractional Fourier transform, the fractional Hilbert transform, and the wavelet\\ntransform....\\nURL: http://arxiv.org/abs/1605.08683v1\\n\\n\\n[4] Identities for the Ln-transform, the L2n-transform and the P2n transform and their applications (2014-03-10)\\nAuthors: Nese Dernek, Fatih Aylikci\\nAbstract: In the present paper, the authors introduce several new integral transforms\\nincluding the Ln-transform, the L2n-transform and P2n-transform generalizations\\nof the classical Laplace transform and the classical Stieltjes transform as\\nrespectively. It is shown that the second iterate of the L2n-transform is\\nessentially the P2n-transform. Using this relationship, a few new\\nParseval-Goldstein type identities are obtained. The theorem and the lemmas\\nthat are proven in this article are new useful relat...\\nURL: http://arxiv.org/abs/1403.2188v1\\n\\n\\n[5] Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks (2022-04-16)\\nAuthors: Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan Cao, Yongjian Wu, Feiyue Huang, Rongrong Ji\\nAbstract: Despite the exciting performance, Transformer is criticized for its excessive\\nparameters and computation cost. However, compressing Transformer remains as an\\nopen problem due to its internal complexity of the layer designs, i.e.,\\nMulti-Head Attention (MHA) and Feed-Forward Network (FFN). To address this\\nissue, we introduce Group-wise Transformation towards a universal yet\\nlightweight Transformer for vision-and-language tasks, termed as\\nLW-Transformer. LW-Transformer applies Group-wise Transforma...\\nURL: http://arxiv.org/abs/2204.07780v1\\n\\n\", name='arxiv1', id='1c180052-ca51-4ce9-8278-d5836222d39f', tool_call_id='call_ao1MjJ7fSCGPvtbAkykWBiEz'),\n",
       "  HumanMessage(content='Here are some research papers related to \"transformer\":\\n\\n1. [The Xi-transform for conformally flat space-time](http://arxiv.org/abs/gr-qc/0612006v1) by George Sparling (2006): This paper discusses the Xi-transform, a new spinor transform arising naturally in Einstein\\'s general relativity. The example of conformally flat space-time is discussed in detail.\\n\\n2. [Multiple basic hypergeometric transformation formulas arising from the balanced duality transformation](http://arxiv.org/abs/1310.1984v2) by Yasushi Kajihara (2013): This paper discusses some multiple hypergeometric transformation formulas arising from the balanced duality transformation formula.\\n\\n3. [The Fourier and Hilbert transforms under the Bargmann transform](http://arxiv.org/abs/1605.08683v1) by Xing-Tang Dong, Kehe Zhu (2016): This paper studies the action of the Bargmann transform on several classical integral operators on L^2(R), including the fractional Fourier transform, the fractional Hilbert transform, and the wavelet transform.\\n\\n4. [Identities for the Ln-transform, the L2n-transform and the P2n transform and their applications](http://arxiv.org/abs/1403.2188v1) by Nese Dernek, Fatih Aylikci (2014): This paper introduces several new integral transforms including the Ln-transform, the L2n-transform and P2n-transform generalizations of the classical Laplace transform and the classical Stieltjes transform.\\n\\n5. [Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks](http://arxiv.org/abs/2204.07780v1) by Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yan Wang, Liujuan Cao, Yongjian Wu, Feiyue Huang, Rongrong Ji (2022): This paper introduces Group-wise Transformation towards a universal yet lightweight Transformer for vision-and-language tasks, termed as LW-Transformer. LW-Transformer applies Group-wise Transformation to both MHA and FFN, significantly reducing the parameters and computation cost.\\n\\nPlease note that the first four papers are not directly related to the Transformer model in machine learning. The last paper is the most relevant to the Transformer model in the context of machine learning and natural language processing.', additional_kwargs={}, response_metadata={}, name='researcher', id='7377d24c-27a8-419a-a6ff-f66f41ebc6dc'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Nv3ZjYVAJySPYRQ8Mrb5vh6T', 'function': {'arguments': '{\\n\"abstract\": \"Despite the exciting performance, Transformer is criticized for its excessive parameters and computation cost. However, compressing Transformer remains as an open problem due to its internal complexity of the layer designs, i.e., Multi-Head Attention (MHA) and Feed-Forward Network (FFN). To address this issue, we introduce Group-wise Transformation towards a universal yet lightweight Transformer for vision-and-language tasks, termed as LW-Transformer. LW-Transformer applies Group-wise Transformation to both MHA and FFN, significantly reducing the parameters and computation cost.\"\\n}', 'name': 'summarize_abstract'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 121, 'prompt_tokens': 1456, 'total_tokens': 1577, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BmQxuRejDUB9Ue8VZO2pdm6CIzKBL', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--efe37949-8bc4-409b-844a-749675989988-0', tool_calls=[{'name': 'summarize_abstract', 'args': {'abstract': 'Despite the exciting performance, Transformer is criticized for its excessive parameters and computation cost. However, compressing Transformer remains as an open problem due to its internal complexity of the layer designs, i.e., Multi-Head Attention (MHA) and Feed-Forward Network (FFN). To address this issue, we introduce Group-wise Transformation towards a universal yet lightweight Transformer for vision-and-language tasks, termed as LW-Transformer. LW-Transformer applies Group-wise Transformation to both MHA and FFN, significantly reducing the parameters and computation cost.'}, 'id': 'call_Nv3ZjYVAJySPYRQ8Mrb5vh6T', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1456, 'output_tokens': 121, 'total_tokens': 1577, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='first=PromptTemplate(input_variables=[\\'abstract\\'], input_types={}, partial_variables={}, template=\\'\\\\n    You are an AI research assistant. Read the abstract below and summarize the key points.\\\\n\\\\n    Abstract:\\\\n    \"{abstract}\"\\\\n\\\\n    Summarize in 3 bullet points.\\\\n    \\') middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000235EB6AC680>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000235EBB52630>, root_client=<openai.OpenAI object at 0x00000235E8569BB0>, root_async_client=<openai.AsyncOpenAI object at 0x00000235EBB20D70>, model_name=\\'gpt-4\\', temperature=0.2, model_kwargs={}, openai_api_key=SecretStr(\\'**********\\'))', name='summarize_abstract', id='3427366e-8947-4998-96bc-503001c0e426', tool_call_id='call_Nv3ZjYVAJySPYRQ8Mrb5vh6T'),\n",
       "  HumanMessage(content=\"- The paper addresses the issue of the Transformer model's excessive parameters and computation cost, which has been a challenge due to the internal complexity of its layer designs, specifically Multi-Head Attention (MHA) and Feed-Forward Network (FFN).\\n- To solve this problem, the authors introduce a Group-wise Transformation approach, aiming to create a universal yet lightweight Transformer for vision-and-language tasks, which they call LW-Transformer.\\n- The LW-Transformer applies Group-wise Transformation to both MHA and FFN, which significantly reduces the parameters and computation cost.\", additional_kwargs={}, response_metadata={}, name='summarizer', id='120f20f5-7290-40ed-91ec-f56810cd4d11'),\n",
       "  HumanMessage(content='The paper \"Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks\" introduces a new approach to address the issue of the Transformer model\\'s high computational cost and excessive parameters. The key points are:\\n\\n- The authors identify the complexity of the Transformer\\'s layer designs, specifically Multi-Head Attention (MHA) and Feed-Forward Network (FFN), as the main challenge in reducing its computational cost and parameters.\\n- They propose a Group-wise Transformation approach to create a more efficient version of the Transformer for vision-and-language tasks, which they call the LW-Transformer.\\n- The LW-Transformer applies Group-wise Transformation to both MHA and FFN, resulting in a significant reduction in parameters and computational cost.', additional_kwargs={}, response_metadata={}, name='researcher', id='598b7e83-eeef-4d29-9786-5382c5101f3e'),\n",
       "  HumanMessage(content='FINAL ANSWER:\\n\\nThe paper \"Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks\" introduces a new approach to address the issue of the Transformer model\\'s high computational cost and excessive parameters. The key points are:\\n\\n- The authors identify the complexity of the Transformer\\'s layer designs, specifically Multi-Head Attention (MHA) and Feed-Forward Network (FFN), as the main challenge in reducing its computational cost and parameters.\\n- They propose a Group-wise Transformation approach to create a more efficient version of the Transformer for vision-and-language tasks, which they call the LW-Transformer.\\n- The LW-Transformer applies Group-wise Transformation to both MHA and FFN, resulting in a significant reduction in parameters and computational cost.', additional_kwargs={}, response_metadata={}, name='summarizer', id='8ef53cb7-9783-4774-a392-a456d9106ad5')]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"messages\": [(\"user\", \"get the research paper in transformer\")], })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952322fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autores-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
